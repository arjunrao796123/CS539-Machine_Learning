{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.read_csv('cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.drop(['Unnamed: 0'],axis=1)\n",
    "#s3 = s3.loc[:,s3.columns!='target']\n",
    "X =s.drop(['WORKSITE_STATE', 'WORKSITE_COUNTY_BIN','H1B_DEPENDENT',\n",
    "       'AGENT_ATTORNEY_STATE_BIN', 'AMENDED_PETITION_BIN','NAICS_CODE_BIN',\n",
    "       'AGENT_REPRESENTING_EMPLOYER','WAGE_UNIT_OF_PAY','EMPLOYER_CITY_BIN',\n",
    "       'NEW_EMPLOYMENT_BIN', 'CHANGE_PREVIOUS_EMPLOYMENT_BIN','Visa_duration_bin','Target'],axis=1)\n",
    "#X=X.drop(['Target'],axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampling and undersampling\n",
    "s4=s.loc[s['Target']==0]\n",
    "s4 = s4.sample(n=50000)#downsampling\n",
    "s5 = s.loc[s['Target']==1]\n",
    "s5 = s5.head(n=1000)\n",
    "s4 = pd.concat([s4,s5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampling\n",
    "s4 = pd.concat([s4,s5])\n",
    "s4 = pd.concat([s4,s5])\n",
    "s4 = pd.concat([s4,s5])\n",
    "s4 = pd.concat([s4,s5])\n",
    "s4 = pd.concat([s4,s5])\n",
    "s4 = pd.concat([s4,s5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(s4.drop(['Target'],axis=1))#do this when sampling\n",
    "#X =pd.get_dummies(X)#do this otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = s4['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Random data testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampling the train data # another way to oversample\n",
    "#uncomment to try\n",
    "#X_train['Target'] = y_train\n",
    "#X_train_2 = pd.concat([X_train,X_train_1])\n",
    "#X_train_2 = pd.concat([X_train_2,X_train_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_2 = pd.concat([X_train_2,X_train_1])\n",
    "#X_train_2 = pd.concat([X_train_2,X_train_1])\n",
    "#X_train_2 = pd.concat([X_train_2,X_train_1])\n",
    "#X_train_2 = pd.concat([X_train_2,X_train_1])\n",
    "#X_train_2 = pd.concat([X_train_2,X_train_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_2 = X_train_2.sample(n=50000)\n",
    "#X_train = X_train_2.drop(['Target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train= X_train_2['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "#logisticRegr.score(test_img, y_test)\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(200,), max_iter=100,learning_rate = 'adaptive',solver='sgd')\n",
    "mlp.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mlp = mlp.predict(X_test)\n",
    "accuracy_score(y_test,pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find the best paramerters\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the confusion martrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve,auc,r2_score\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt = \"d\",linecolor=\"k\",linewidths=2)\n",
    "plt.title(\"CONFUSION MATRIX\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plottting the ROC curve\n",
    "predicting_probabilites = mlp.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "fpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\n",
    "plt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\n",
    "plt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since denial rate is 1%, we tried with a threshold around 1% to get a rough estimate\n",
    "import numpy as np\n",
    "THRESHOLD = 0.015\n",
    "preds = np.where(logisticRegr.predict_proba(X_test)[:,1] > THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing PCA \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca=PCA(0.8)\n",
    "pca.fit(X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA transformation\n",
    "X_train_1 = pca.transform(X_train)\n",
    "X_test_1 = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing SVC after PCA\n",
    "from sklearn.svm import SVC\n",
    "svclassifer7 = SVC(C=10,kernel='rbf',gamma='scale',probability=True)\n",
    "svclassifer7.fit(X_train_1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svc7 = svclassifer7.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Keras libraries and packages\n",
    "#Tried on tensorflow/keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten\n",
    "import numpy as np\n",
    "y4 = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "opt = SGD(lr=0.001)\n",
    "# the shape of one training example is\n",
    "\n",
    "model.add(Dense(units=60, activation='sigmoid',kernel_initializer='glorot_uniform', input_shape=(107,)))\n",
    "\n",
    "model.add(Dense(units=50, activation='sigmoid'))\n",
    "model.add(Dense(units=30, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=10, activation='relu'))\n",
    "#model.add(Dense(units=5, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "model.fit(X_train, y4, epochs=100,batch_size=200,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nn = model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro = np.round(pred_nn)\n",
    "#then take the model and perform ROC curve and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
